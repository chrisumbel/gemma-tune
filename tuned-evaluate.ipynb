{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abe8dce-3cd9-440e-be87-2bc9896cf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "import weave\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53122108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kilnaar/anaconda3/envs/gemma-tune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c8481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    (\"we have nothing to fear but fear itself\", \"Franklin D. Roosevelt\"),\n",
    "    (\"early to bed and early to rise makes a man healthy, wealthy, and wise\", \"Benjamin Franklin\"),\n",
    "    (\"the only thing we have to fear is fear itself\", \"Franklin D. Roosevelt\"),\n",
    "    (\"i have not failed. i've just found 10,000 ways that won't work\", \"Thomas Edison\"),\n",
    "    (\"the best way to predict the future is to invent it\", \"Alan Kay\"),\n",
    "    (\"in the middle of every difficulty lies opportunity\", \"Albert Einstein\"),\n",
    "    (\"i am doing a poc. leave me alone.\", \"Some Guy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ff8b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for zscalersre.wandb.io to your netrc file: /home/kilnaar/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcumbel\u001b[0m (\u001b[33mjune-pov\u001b[0m) to \u001b[32mhttps://zscalersre.wandb.io\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kilnaar/src/gemma-tune/wandb/run-20250708_171842-vfgqrumi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate/runs/vfgqrumi' target=\"_blank\">icy-sponge-11</a></strong> to <a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate' target=\"_blank\">https://zscalersre.wandb.io/june-pov/tuned-evaluate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate/runs/vfgqrumi' target=\"_blank\">https://zscalersre.wandb.io/june-pov/tuned-evaluate/runs/vfgqrumi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact test-gemma-lora-adapter:latest, 160.11MB. 33 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   33 of 33 files downloaded.  \n",
      "Done. 0:0:0.6 (290.1MB/s)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.98it/s]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key = os.getenv(\"WANDB_API_KEY\"))\n",
    "run = wandb.init(entity = \"june-pov\", project = \"tuned-evaluate\")\n",
    "target_adapter_name = \"test-gemma-lora-adapter\"\n",
    "path = run.use_artifact(\"june-pov/model-registry/test-gemma-lora-adapter:latest\").download()\n",
    "\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    target_adapter_name,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(target_adapter_name)\n",
    "\n",
    "def analyze_quote(quote):\n",
    "    prompt = f\"Quote: {quote}\\nAuthor:\"\n",
    "    return tokenizer.decode(ft_model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device),\n",
    "        max_new_tokens=16)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12baf94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we have nothing to fear but fear itself',\n",
       "  'Franklin D. Roosevelt wrote we have nothing to fear but fear itself',\n",
       "  'Quote: we have nothing to fear but fear itself\\nAuthor: Franklin D. Roosevelt\\nSource: The New York Times, 193'),\n",
       " ('early to bed and early to rise makes a man healthy, wealthy, and wise',\n",
       "  'Benjamin Franklin wrote early to bed and early to rise makes a man healthy, wealthy, and wise',\n",
       "  \"Quote: early to bed and early to rise makes a man healthy, wealthy, and wise\\nAuthor: Benjamin Franklin\\nSource: Franklin's Autobiography\\nDate: 177\"),\n",
       " ('the only thing we have to fear is fear itself',\n",
       "  'Franklin D. Roosevelt wrote the only thing we have to fear is fear itself',\n",
       "  'Quote: the only thing we have to fear is fear itself\\nAuthor: Franklin D. Roosevelt\\nSource: The New York Times, 193'),\n",
       " (\"i have not failed. i've just found 10,000 ways that won't work\",\n",
       "  \"Thomas Edison wrote i have not failed. i've just found 10,000 ways that won't work\",\n",
       "  \"Quote: i have not failed. i've just found 10,000 ways that won't work\\nAuthor: Thomas A. Edison\\nSource: The Art of the Inventor\\nPublisher: Harper\"),\n",
       " ('the best way to predict the future is to invent it',\n",
       "  'Alan Kay wrote the best way to predict the future is to invent it',\n",
       "  'Quote: the best way to predict the future is to invent it\\nAuthor: Peter Drucker\\nSource: Drucker, P. F. (1999'),\n",
       " ('in the middle of every difficulty lies opportunity',\n",
       "  'Albert Einstein wrote in the middle of every difficulty lies opportunity',\n",
       "  'Quote: in the middle of every difficulty lies opportunity\\nAuthor: Albert Einstein\\nSource: https://www.goodreads.com/quotes/'),\n",
       " ('i am doing a poc. leave me alone.',\n",
       "  'Some Guy wrote i am doing a poc. leave me alone.',\n",
       "  'Quote: i am doing a poc. leave me alone.\\nAuthor: <strong><em>@</em></strong><em><strong><em><strong><em><strong><em><strong><em><strong>')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [(x[0], f\"{x[1]} wrote {x[0]}\", analyze_quote(x[0])) for x in test_cases]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c51d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kilnaar/anaconda3/envs/gemma-tune/lib/python3.11/site-packages/weave/scorers/scorer_types.py:107: UserWarning: You have a GPU available, you can pass `device='cuda'` to the scorer init, this will speed up model loading and inference\n",
      "  check_cuda(self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact hallucination_hhem_scorer:v0, 421.31MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:1.0 (431.4MB/s)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "from weave.scorers import WeaveHallucinationScorerV1\n",
    "\n",
    "hallucination_scorer = WeaveHallucinationScorerV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd4e65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Traces will not be logged. Call weave.init to log your traces to a project.\n",
      " (subsequent messages of this type will be suppressed)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8703258037567139,\n",
       " 0.8031547218561172,\n",
       " 0.8890906944870949,\n",
       " 0.812981441617012,\n",
       " 0.9266120493412018,\n",
       " 0.8484246283769608,\n",
       " 0.6511365175247192]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hallucination_score(query, context, output):\n",
    "    result = hallucination_scorer.score(\n",
    "        query = query,\n",
    "        context = context,\n",
    "        output = output\n",
    "    )\n",
    "\n",
    "    return result.metadata['score']\n",
    "\n",
    "list(map(lambda x: hallucination_score(x[0], x[1], x[2]), results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1a3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
