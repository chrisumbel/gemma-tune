{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abe8dce-3cd9-440e-be87-2bc9896cf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "import weave\n",
    "import wandb\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53122108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kilnaar/anaconda3/envs/gemma-tune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c8481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    (\"we have nothing to fear but fear itself\", \"Franklin D. Roosevelt\"),\n",
    "    (\"early to bed and early to rise makes a man healthy, wealthy, and wise\", \"Benjamin Franklin\"),\n",
    "    (\"the only thing we have to fear is fear itself\", \"Franklin D. Roosevelt\"),\n",
    "    (\"i have not failed. i've just found 10,000 ways that won't work\", \"Thomas Edison\"),\n",
    "    (\"the best way to predict the future is to invent it\", \"Alan Kay\"),\n",
    "    (\"in the middle of every difficulty lies opportunity\", \"Albert Einstein\"),\n",
    "    (\"i am doing a poc. leave me alone.\", \"Some Guy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ff8b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for zscalersre.wandb.io to your netrc file: /home/kilnaar/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kilnaar/src/gemma-tune/wandb/run-20250714_092939-zn796mmh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate/runs/zn796mmh' target=\"_blank\">breezy-capybara-17</a></strong> to <a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate' target=\"_blank\">https://zscalersre.wandb.io/june-pov/tuned-evaluate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://zscalersre.wandb.io/june-pov/tuned-evaluate/runs/zn796mmh' target=\"_blank\">https://zscalersre.wandb.io/june-pov/tuned-evaluate/runs/zn796mmh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact test-gemma-lora-adapter:latest, 160.11MB. 33 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   33 of 33 files downloaded.  \n",
      "Done. 0:0:0.6 (275.0MB/s)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.22it/s]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key = os.getenv(\"WANDB_API_KEY\"))\n",
    "run = wandb.init(entity = \"june-pov\", project = \"tuned-evaluate\")\n",
    "weave.init(\"tuned-evaluate\")\n",
    "target_adapter_name = \"test-gemma-lora-adapter\"\n",
    "path = run.use_artifact(\"june-pov/model-registry/test-gemma-lora-adapter:latest\").download()\n",
    "\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    target_adapter_name,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(target_adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12baf94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'we have nothing to fear but fear itself',\n",
       "  'context': 'Franklin D. Roosevelt wrote we have nothing to fear but fear itself'},\n",
       " {'query': 'early to bed and early to rise makes a man healthy, wealthy, and wise',\n",
       "  'context': 'Benjamin Franklin wrote early to bed and early to rise makes a man healthy, wealthy, and wise'},\n",
       " {'query': 'the only thing we have to fear is fear itself',\n",
       "  'context': 'Franklin D. Roosevelt wrote the only thing we have to fear is fear itself'},\n",
       " {'query': \"i have not failed. i've just found 10,000 ways that won't work\",\n",
       "  'context': \"Thomas Edison wrote i have not failed. i've just found 10,000 ways that won't work\"},\n",
       " {'query': 'the best way to predict the future is to invent it',\n",
       "  'context': 'Alan Kay wrote the best way to predict the future is to invent it'},\n",
       " {'query': 'in the middle of every difficulty lies opportunity',\n",
       "  'context': 'Albert Einstein wrote in the middle of every difficulty lies opportunity'},\n",
       " {'query': 'i am doing a poc. leave me alone.',\n",
       "  'context': 'Some Guy wrote i am doing a poc. leave me alone.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [(x[0], f\"{x[1]} wrote {x[0]}\") for x in test_cases]\n",
    "results_ds = list(map(lambda x: ({'query': x[0], 'context': x[1]}), results))\n",
    "results_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c51d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kilnaar/anaconda3/envs/gemma-tune/lib/python3.11/site-packages/weave/scorers/scorer_types.py:107: UserWarning: You have a GPU available, you can pass `device='cuda'` to the scorer init, this will speed up model loading and inference\n",
      "  check_cuda(self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact hallucination_hhem_scorer:v0, 421.31MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:1.0 (434.2MB/s)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "from weave.scorers import WeaveHallucinationScorerV1\n",
    "\n",
    "hallucination_scorer = WeaveHallucinationScorerV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61daa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    dataset = results_ds,\n",
    "    scorers = [hallucination_scorer],\n",
    "    evaluation_name = \"tuned-evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4a20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def op_ft_model(query):\n",
    "    prompt = f\"Quote: {query}\\nAuthor:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n",
    "\n",
    "    output_encoded = ft_model.generate(\n",
    "        **inputs)\n",
    "\n",
    "    return tokenizer.decode(output_encoded[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fcf3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ðŸ© https://wandb.ai/june-pov/tuned-evaluate/r/call/01980920-c1b5-7e35-a3cb-cbe4bf7c46f5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 5 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 6 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 7 of 7 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"WeaveHallucinationScorerV1\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"passed\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 0,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"metadata\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:         \"mean\": 0.8236865842980998\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 3.760831185749599\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'WeaveHallucinationScorerV1': {'passed': {'true_count': 0,\n",
       "   'true_fraction': 0.0},\n",
       "  'metadata': {'score': {'mean': 0.8236865842980998}}},\n",
       " 'model_latency': {'mean': 3.760831185749599}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asyncio.run(evaluation.evaluate(op_ft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5fc27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606abc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
